# Local Document Q&A Assistant (Multimodal + Web Search)

This application allows you to upload documents (PDFs, TXTs), ask questions about their content (including text, tables, and figures within PDFs if enabled), and receive answers generated by powerful AI language models (OpenAI or Google Gemini). You can configure various settings to control how the AI retrieves information and responds, and optionally include web search results.

## Features

*   **Document Upload:** Supports PDF and TXT files. (PDF required for image analysis).
*   **OCR:** Automatically performs Optical Character Recognition on scanned PDFs to extract text.
*   **Text Indexing:** Creates a searchable vector index of the text content using FAISS.
*   **Incremental Processing:** Only processes documents that haven't been indexed previously in the session.
*   **Multimodal Q&A (Optional):** Can analyze text, tables, figures, and graphs within PDFs using vision-capable LLMs (GPT-4o/Turbo Vision, Gemini 1.5 Pro).
*   **Web Search Integration (Optional):** Can perform a web search based on your query and include results in the context provided to the AI.
*   **Configurable AI:**
    *   Choose between available OpenAI or Google Gemini models (requires API keys).
    *   Adjust LLM Temperature (creativity vs. factuality).
    *   Select Text Retrieval Mode (Similarity Search vs. MMR for diversity).
    *   Configure MMR parameters (`k`, `fetch_k`, `lambda`).
    *   Edit the core instructions (system prompt) given to the AI.
*   **Context Management:** Clear processed documents and index easily.
*   **Local Interface:** Runs as a web application on your local machine using Streamlit.

## Prerequisites

Before you begin, ensure you have the following installed on your system:

1.  **Python:** Version 3.9 or higher recommended.
2.  **System Dependencies (from `packages.txt`):** These non-Python tools are required for OCR and PDF handling. Install them using your system's package manager:
    *   **Tesseract OCR Engine & English Language Pack:**
        *   *macOS (using Homebrew):*
            ```bash
            brew install tesseract tesseract-lang
            ```
        *   *Debian/Ubuntu Linux:*
            ```bash
            sudo apt-get update && sudo apt-get install -y tesseract-ocr tesseract-ocr-eng
            ```
        *   *Windows:* Download Tesseract installer from the [official docs](https://github.com/tesseract-ocr/tessdoc) and ensure it's added to your system's PATH.
    *   **Poppler Utilities:**
        *   *macOS (using Homebrew):*
            ```bash
            brew install poppler
            ```
        *   *Debian/Ubuntu Linux:*
            ```bash
            sudo apt-get update && sudo apt-get install -y poppler-utils
            ```
        *   *Windows:* Download Poppler binaries (e.g., from [Poppler for Windows releases](https://github.com/oschwartz10612/poppler-windows/releases/)) and add its `bin/` directory to your system's PATH.

## Setup Instructions

1.  **Get the Code:** Place all the project files (`streamlit_app_local.py`, `requirements.txt`, etc.) into a single directory (e.g., `your_qa_app`).
2.  **Navigate to Directory:** Open your terminal or command prompt and change into the project directory:
    ```bash
    cd path/to/your_qa_app
    ```
3.  **Create Virtual Environment:** It's highly recommended to use a virtual environment to isolate dependencies.
    ```bash
    python3 -m venv venv
    ```
    *(On some systems, you might just use `python` instead of `python3`)*
4.  **Activate Virtual Environment:**
    *   **macOS/Linux:** `source venv/bin/activate`
    *   **Windows (Command Prompt):** `.\venv\Scripts\activate.bat`
    *   **Windows (PowerShell):** `.\venv\Scripts\Activate.ps1`
    *(Your terminal prompt should change, often showing `(venv)` at the beginning)*
5.  **Install Python Requirements:** Install all necessary Python libraries from the `requirements.txt` file.
    ```bash
    pip install -r requirements.txt
    ```
    *(This might take a few minutes)*
6.  **Configure API Keys:**
    *   You need API keys for the AI models you want to use (OpenAI and/or Google AI).
    *   Create a file named `.env` **in the main project directory** (`your_qa_app/`).
    *   Add your keys to the `.env` file, one per line, like this (replace placeholders with your actual keys):
        ```dotenv
        OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxx
        GOOGLE_API_KEY=AIzaxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
        ```
    *   **IMPORTANT:** Keep this `.env` file secure and do not share it, as it contains your secret keys.
    *   Get OpenAI keys: [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    *   Get Google AI (Gemini) keys: [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)

## Running the Application

1.  **Ensure Virtual Environment is Active:** You should see `(venv)` in your terminal prompt. If not, activate it (see Setup step 4).
2.  **Run Streamlit:** Execute the following command from the project directory:
    ```bash
    streamlit run streamlit_app_local.py
    ```
3.  **Access the App:** Your default web browser should automatically open to the local web application (usually at `http://localhost:8501`).

## Usage Guide

1.  **Configure (Sidebar - Left):**
    *   **Select LLM:** Choose between OpenAI or Google (only options for which you provided API keys will appear). Use models marked "(Vision)" for image analysis.
    *   **Adjust Temperature:** Control AI creativity.
    *   **Enable/Disable Features:** Use the checkboxes to turn "Image Analysis" (requires Vision LLM and PDFs) and "Web Search" on or off for your queries.
    *   **Set Retrieval:** Choose "Similarity Search" or "MMR". Adjust parameters (`k`, `fetch_k`, `lambda`) as needed using the sliders/number inputs. Hover over the `?` icons for explanations.
    *   **Edit Instructions (Optional):** Modify the text in the "AI Instructions" box to change how the AI behaves. Use `{context}` and `{question}` placeholders.
2.  **Upload Documents (Sidebar):**
    *   Click "Browse files" under "Document Management".
    *   Select one or more PDF files (required for image analysis) or TXT files (if image analysis is disabled).
3.  **Process Documents (Sidebar):**
    *   Click the "Process Uploaded Documents" button.
    *   Wait for the processing to complete (check the status messages in the main app area). This includes OCR for scanned PDFs and can take time for large files. The app only processes files it hasn't seen before in the current index.
4.  **Ask Questions (Main Area):**
    *   Once processing is done and the LLM is configured, type your question or task into the text box at the bottom of the main area and press Enter.
5.  **View Response:**
    *   The AI's response will appear in the chat area.
    *   Below the response, you can expand the "Sources Considered" section to see which document pages (text and/or images) and web results (if enabled) were used as context.
6.  **Clear Context (Sidebar):**
    *   Click "Clear All Data" to remove the index and forget all processed documents/images. You will need to re-process files after clearing.

## Notes

*   **API Costs:** Using OpenAI and Google AI models incurs costs based on usage, usually measured in tokens processed. Check their respective pricing pages. Web search might also have implicit costs/limits.
*   **Local Index:** The document index (`faiss_index_local_mm/`) is stored locally on your computer. Clearing context deletes this folder.
*   **Image Cache:** Generated images are stored temporarily in the app's memory during a session (`st.session_state`). Processing many large PDFs might consume significant RAM. This cache is cleared when you click "Clear All Data" or stop the Streamlit app.
*   **Performance:** OCR and processing large documents can be slow. Queries involving web search or complex analysis will also take longer.